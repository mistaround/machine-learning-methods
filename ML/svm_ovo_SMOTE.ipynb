{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('m_train_ori.csv',delimiter=',')\n",
    "testset = np.loadtxt('m_test_ori.csv',delimiter=',')\n",
    "\n",
    "dataset_all = dataset[0:dataset.shape[0],0:dataset.shape[1]-2]\n",
    "dataset_all = preprocessing.scale(dataset_all)\n",
    "dataset_all_label = dataset[0:dataset.shape[0],dataset.shape[1]-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6835443037974683\n",
      "0.13291139240506328\n"
     ]
    }
   ],
   "source": [
    "dataset1 = []\n",
    "dataset2 = []\n",
    "dataset3 = []\n",
    "dataset4 = []\n",
    "dataset5 = []\n",
    "\n",
    "for r in range(dataset.shape[0]):\n",
    "    if dataset[r,dataset.shape[1]-2] == 1:\n",
    "        dataset1.append(dataset[r])\n",
    "    elif dataset[r,dataset.shape[1]-2] == 2:\n",
    "        dataset2.append(dataset[r])\n",
    "    elif dataset[r,dataset.shape[1]-2] == 3:\n",
    "        dataset3.append(dataset[r])\n",
    "    elif dataset[r,dataset.shape[1]-2] == 4:\n",
    "        dataset4.append(dataset[r])\n",
    "    elif dataset[r,dataset.shape[1]-2] == 5:\n",
    "        dataset5.append(dataset[r])\n",
    "        \n",
    "dataset1 = np.array(dataset1)\n",
    "dataset2 = np.array(dataset2)\n",
    "dataset3 = np.array(dataset3)\n",
    "dataset4 = np.array(dataset4)\n",
    "dataset5 = np.array(dataset5)\n",
    "\n",
    "datasets = [dataset1, dataset2, dataset3, dataset4, dataset5]\n",
    "\n",
    "sample_num = max([dataset1.shape[0], dataset2.shape[0], dataset3.shape[0], dataset4.shape[0], dataset5.shape[0]])\n",
    "\n",
    "for k in range(len(datasets)):\n",
    "    if datasets[k].shape[0] != sample_num:\n",
    "        cur_num = datasets[k].shape[0]\n",
    "        sample_rate = int(sample_num / cur_num)\n",
    "        neigh = NearestNeighbors(n_neighbors = 2)\n",
    "        neigh.fit(datasets[k])\n",
    "        for i in range(cur_num):\n",
    "            neis = neigh.kneighbors([datasets[k][i]])\n",
    "            for j in range(sample_rate):\n",
    "                l = random.randrange(2)\n",
    "                new_row = datasets[k][i] + random.random() * (datasets[k][neis[1][0][l]] - datasets[k][i])\n",
    "                datasets[k] = np.vstack((datasets[k], new_row))\n",
    "                \n",
    "#print(datasets[4].shape)\n",
    "\n",
    "test_row = testset.shape[0]\n",
    "test_col = testset.shape[1]\n",
    "\n",
    "test = testset[0:test_row,0:test_col-2]\n",
    "test = preprocessing.scale(test)\n",
    "\n",
    "test_label_1 = testset[0:test_row,test_col-2]\n",
    "\n",
    "labels = []\n",
    "labels_train = []\n",
    "judge = 1\n",
    "judge_train = 1\n",
    "\n",
    "for i in range(len(datasets) - 1):\n",
    "    for j in range(i + 1,len(datasets)):\n",
    "        dataset = np.vstack((datasets[i],datasets[j]))\n",
    "\n",
    "        row = dataset.shape[0]\n",
    "        col = dataset.shape[1]\n",
    "        \n",
    "        pca = PCA(n_components = 5)\n",
    "        train = dataset[0:row,0:col-2]\n",
    "        train = preprocessing.scale(train)\n",
    "        train = pca.fit_transform(train)\n",
    "        \n",
    "        train_label_1 = dataset[0:row,col-2]\n",
    "        \n",
    "        clf = svm.SVC()\n",
    "        clf.fit(train, train_label_1)\n",
    "        test_tmp = pca.transform(test)\n",
    "        dataset_temp = pca.transform(dataset_all)\n",
    "        \n",
    "        if judge_train == 1:\n",
    "            labels_train = np.array([clf.predict(dataset_temp)]).T\n",
    "            judge_train = 0\n",
    "        else:\n",
    "            labels_train = np.hstack((labels_train,np.array([clf.predict(dataset_temp)]).T))\n",
    "        \n",
    "        if judge == 1:\n",
    "            labels = np.array([clf.predict(test_tmp)]).T\n",
    "            judge = 0\n",
    "        else:\n",
    "            labels = np.hstack((labels,np.array([clf.predict(test_tmp)]).T))\n",
    "            \n",
    "labels = labels.astype(np.int64)\n",
    "\n",
    "labels_train = labels_train.astype(np.int64)\n",
    "c = 0\n",
    "for i in range(labels.shape[0]):\n",
    "    if np.argmax(np.bincount(labels[i])) != int(test_label_1[i]):\n",
    "        #print(i,np.argmax(np.bincount(labels[i])),int(test_label_1[i]))\n",
    "        c += 1\n",
    "print(1-c/test_row)\n",
    "\n",
    "c = 0\n",
    "for i in range(labels_train.shape[0]):\n",
    "    if np.argmax(np.bincount(labels_train[i])) != int(dataset_all_label[i]):\n",
    "        #print(i,np.argmax(np.bincount(labels[i])),int(test_label_1[i]))\n",
    "        c += 1\n",
    "print(c/labels_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
